# 阶段2

## 目标：

记得同步到git上

创新点：

* Get track histories of all neighbours in the grid

  ！！之后可以改，加入其他车的未来的轨迹进输入特征里，相当于矫正！！！现在neighbour只看了历史的,相当于套娃了，用预测的未来轨迹（还是真实的未来轨迹，在train/test时用的一样吗？）
* 坐标系: 都用的相对坐标，local_x,local_y还是在这段记录区域内的全局绝对坐标，转成以自车为原点(0,0)的相对坐标，-refPos(自车的坐标)

1. 2022.12.1-12.2：

   - [X] 不改preprocess_data.m，直接用mat，首先看懂data
   - [X] 写出data的类型，维度，写成实验报告！！！
2. 2022.12.5：

- [X] utils.py train.py model.py 看懂写好注释

3. 12.6

   - [ ] 传到git
   - [ ] 改train，model.py记录数据成表格，改成加上tqdm和用tensorboard画loss！！！
   - [ ] 读nn.transformer的底层代码
   - [ ] 预处理写一小节！！！，把preprocess_data.m里的写出来！！
     1. 坐标系选取
     2. 加的标签方法
     3. 输入形式，建模数学表达式，以及具体矩阵维度
4. 2022.12.5-12.9：

   - [ ] 只改网络，用nn.transformer改；
   - [ ] 进nn.transformer的底层代码改，然后看李宏毅的章节4、5的有一个讲到各种former变形，放一个在这里试一试
   - [ ] 预处理写一小节！！！，把preprocess_data.m里的写出来！！
     - [ ] 坐标系选取
     - [ ] 加的标签方法
     - [ ] 输入形式，建模数学表达式，以及具体矩阵维度
5. 2022.12.10-12.15：

   - [ ] 可视化
     - [ ] loss
     - [ ] 车轨迹的结果）
   - [ ] 表格

## 问题：

#### 2022

- [ ] 12.5：
  - [ ] utils.py里torch.ones().byte()
  - [ ] train.py里3 train里一开始pretrain用的mse ,后面train用的nLL（用到了BCELoss）为什么？？？注释说的是：Pre-train with MSE loss to speed up training，这里不对的，
- [ ] 12.6
  - [ ] vscode 还是pycharm打开内容多的ipynb有的时候都会莫名其妙无比卡，整个卡死，要开ipynb前一定打开**资源管理器**，随时强行关闭，vscode选自动保存！！！

## 心得：

1. 多按ctrl+放在函数名上，看官方对参数的解释+example，比无脑网上查快+准确，看不懂再网上查（12.5）

   BCE loss: \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]

### 2022.12.1

1. 上一阶段：10.20-11.30 自己想改preprocess改了一圈，不会写网络之类的，存储格式不合理之类的，下面，参考:\2022\prediction\Project1_NGSIM\baseline3.0_csLSTM\conv-social-pooling-master\CS_LSTM_预处理写废了
2. **看preprocess_data.m的解释data每一列的内容**，配合：https://www.jianshu.com/p/d91d6d06766b（两个不一样）
3. 测试data的含义：D:\2022\prediction\Project1_NGSIM\baseline3.0_csLSTM\conv-social-pooling-master\NGSIM_try\debug.ipynb

### 2022.12.5

1. loss.py看https://github.com/nachiket92/conv-social-pooling/issues/7 源代码是错的，错的一塌糊涂，根本没法改，按自己的方法写一个流程！！！
2. fut = vehTrack[stpt:enpt:self.d_s,1:3]**-refPos**#减refPos自车，因为转成**相对坐标**！！！！

# 阶段3

**当中断了一个月，因为对改不出来失望了，转而学习基本功去了（以后知道了，在这期间可以作文献调研，然后接着找能跑的好的开源）**

## 目标：

- [ ] 20231.15-2023.1.29
  - [ ] 对着抄+改正他错误的计算loss方法（不用NLL 看看用crossEntropyLoss和rmse可以吗）
  - [ ] 先只用简单的自车的x,y（不加laneid,v也不加他车的这些东西，后面慢慢加上去），一个最简单的lstm写出来！！
  - [ ] 换成nn.transformer然后改原始库，用图的方法再试试（**attention和图和lstm和conv排列组合**，可以作为好几种方法发单独几篇）
- [ ] 1.16：
  - [ ] 改loss和train 对比orgin 和NGSIM_try里面的改到v3里面去
  - [ ] 做cslstmPPT网络结构和结果即可

## 问题：

#### 2022

- [ ] 12.5：
  - [ ] utils.py里torch.ones().byte()
  - [ ] train.py里3 train里一开始pretrain用的mse ,后面train用的nLL（用到了BCELoss）为什么？？？注释说的是：Pre-train with MSE loss to speed up training，这里不对的，
- [ ] 12.6
  - [ ] vscode 还是pycharm打开内容多的ipynb有的时候都会莫名其妙无比卡，整个卡死，要开ipynb前一定打开**资源管理器**，随时强行关闭，vscode选自动保存！！！
- [ ] 1.15
  - [ ] utils.py的collate_fn替换torch.utils.data 的dataLoader的打包成batch部分没看懂，维数很混乱！！！！

#### 2023

- [ ] 1.15：

  - [ ] train.py里3 train里一开始pretrain用的mse ,后面train用的nLL（用到了BCELoss）为什么？？？注释说的是：Pre-train with MSE loss to speed up training，

    - [ ] 答复：这里不对的，**bce、nll、crossentropy**都是对**分类问题**的，所以当cslstm（M）即基于意图的时候，可以对分类出来的意图用这些指标

      当**没有基于意图的时候，就可以用RMSE来评价回归问题**
- [ ] 2.7：

  - [ ] 没解决1.15

## 心得：

1. 1.15：重点看model里面的def forward(self,x): **标出每层输出的维度**和根据这个改上面参数的维度

# 阶段5

## 目标：

#### 2023年3月

**当中断了2个月，因为对改不出来失望了，转而学习基本功去了（以后知道了，在这期间可以作文献调研，然后接着找能跑的好的开源）** 学了nlp_tutorial的基本seq2seq模型，

- [ ] 先自己写一个最简单的seq2seq模型，即encoder-decoder都是lstm，（loss用RMSE，不考虑周围车辆！！！）
- [ ] 完成可视化就只做这两件事情，慢慢做，脚踏实地，目标定的小一点才做的下去

#### 2023.3.8

- [ ] seq2seq版本写完 preprocess.py不考虑其他车的写完了！！！！ lstm的https://blog.csdn.net/qq_43391414/article/details/120462055
